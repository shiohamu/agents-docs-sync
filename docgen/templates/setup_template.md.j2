{% if has_python or has_javascript or has_go %}
## 前提条件

{% if has_python %}- Python 3.12以上{% endif %}
{% if has_javascript %}- Node.js (推奨バージョン: 18以上){% endif %}
{% if has_go %}- Go 1.21以上{% endif %}

## インストール

{% if has_python %}
### Python
{% if python_pm == "uv" %}
```bash
# uvを使用する場合
uv sync
```
{% elif python_pm == "poetry" %}
```bash
# Poetryを使用する場合
poetry install
```
{% else %}
```bash
pip install -r requirements.txt
```
{% endif %}{% endif %}

{% if has_javascript %}
### JavaScript
{% if js_pm == "yarn" %}
```bash
# Yarnを使用する場合
yarn install
```
{% elif js_pm == "pnpm" %}
```bash
# pnpmを使用する場合
pnpm install
```
{% else %}
```bash
# npmを使用する場合
npm install
```
{% endif %}
{% endif %}

## LLM環境のセットアップ

### APIを使用する場合

1. **APIキーの取得と設定**

   - OpenAI APIキーを取得: https://platform.openai.com/api-keys
   - 環境変数に設定: `export OPENAI_API_KEY=your-api-key-here`

2. **API使用時の注意事項**
   - APIレート制限に注意してください
   - コスト管理のために使用量を監視してください

### ローカルLLMを使用する場合

1. **ローカルLLMのインストール**

   - Ollamaをインストール: https://ollama.ai/
   - モデルをダウンロード: `ollama pull llama3`
   - サービスを起動: `ollama serve`

2. **ローカルLLM使用時の注意事項**
   - モデルが起動していることを確認してください
   - ローカルリソース（メモリ、CPU）を監視してください
{% endif %}
